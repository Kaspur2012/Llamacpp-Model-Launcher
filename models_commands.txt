----- USER-FRIENDLY COMMAND GUIDE -----
-- A curated list of the most important parameters with easy-to-understand explanations and examples.


----- Essential Parameters -----

-m,    --model FNAME
       Specifies the path to the GGUF model file you want to load. This is the most important parameter.
       Example: -m "D:\models\llama-3-8b-instruct.gguf"

-ngl,  --n-gpu-layers N
       [Performance] Offloads a specified number of the model's layers to the GPU's VRAM. This is the primary way to
       accelerate inference speed.
       - To offload all layers, use a high number like 999. The program will automatically stop at the max supported.
       - To use CPU only, set to 0.
       - If you run out of VRAM and the model fails to load, reduce this number until it fits.
       Example: -ngl 999 (Offload as many layers as possible to the GPU)

-c,    --ctx-size N
       Sets the context size (in tokens). This is the maximum amount of text (your prompt + the model's response)
       that the model can "remember" at one time.
       - Larger values require more VRAM/RAM.
       - A common value is 4096 or 8192. Llama 3 models can often use 8192+.
       Example: -c 8192


----- Performance & Memory Tuning -----

-fa,   --flash-attn
       [Performance] Enables Flash Attention, a technique that speeds up processing and can reduce VRAM usage,
       especially with large contexts. It is highly recommended to enable this if your GPU supports it (most modern
       NVIDIA GPUs do).
       Example: -fa on

-t,    --threads N
       Specifies the number of CPU threads to use for processing.
       - A good starting point is the number of physical cores in your CPU.
       - Setting it too high can sometimes hurt performance.
       Example: -t 8

-b,    --batch-size N
       The logical maximum number of tokens to process in a single batch during prompt ingestion.
       - The default is usually fine (e.g., 2048).
       - You might need to lower it if you run into memory errors with very long prompts.
       Example: -b 2048

--mlock
       Locks the model in your system's RAM, preventing the operating system from swapping parts of it to a slower
       disk drive. This can prevent stuttering and slowdowns during generation.

--no-mmap
       Disables memory mapping. The model will be loaded directly into RAM instead of being mapped from the file on
       disk. This leads to a slower startup but can offer more stable performance on some systems, especially if not
       using --mlock.


----- Sampling Parameters (How the model generates text) -----

--temp N
       Temperature. Controls the creativity and randomness of the output.
       - Higher values (e.g., 1.0) make the output more random and "creative".
       - Lower values (e.g., 0.2) make the output more focused, predictable, and deterministic.
       - A value of 0.0 means the model will always pick the single most likely next token (greedy decoding).
       Example: --temp 0.7 (A good balance for creative yet coherent responses)

--top-k N
       Top-K sampling. The model considers only the top K most probable tokens at each step of generation.
       - This prevents the model from picking very unlikely, nonsensical tokens.
       - A value of 40 or 50 is common. Set to 0 to disable.
       Example: --top-k 40

--top-p N
       Top-P (nucleus) sampling. The model considers the smallest set of tokens whose cumulative probability exceeds
       the threshold P. It's an alternative to Top-K.
       - This allows the number of choices to vary dynamically based on the context.
       - A value of 0.9 or 0.95 is common. Set to 1.0 to disable.
       Example: --top-p 0.95

--min-p N
       Min-P sampling. A newer technique that helps curb repetitive and dull text by removing tokens below a certain
       probability threshold, relative to the most likely token.
       - A value of 0.05 or 0.1 is common. Set to 0.0 to disable.
       Example: --min-p 0.05

--repeat-penalty N
       Penalizes the model for repeating the same tokens or phrases, helping to prevent it from getting stuck in loops.
       - A value of 1.1 is a standard starting point.
       - A value of 1.0 disables the penalty.
       Example: --repeat-penalty 1.1


----- Multi-GPU Parameters -----

-mg,   --main-gpu INDEX
       Specifies which GPU to use for crucial calculations and to hold parts of the model that aren't layers.
       GPU indexing starts at 0.
       Example: -mg 0 (Use the first detected GPU for primary work)

-ts,   --tensor-split N0,N1,N2,...
       Manually splits the model's layers across multiple GPUs by proportion. The numbers represent ratios of the
       offloaded layers to be placed on each GPU.
       - The list should have as many numbers as you have GPUs you want to use.
       - The tool sums the numbers and calculates the percentage for each GPU.
       Example: -ts 3,1 (On a 2-GPU system, put 75% of the offloaded layers on GPU 0 and 25% on GPU 1)

-sm,   --split-mode {none,layer,row}
       Determines the strategy for splitting the model across multiple GPUs.
       - none: (Default for 1 GPU) All layers are placed on the --main-gpu.
       - layer: (Default for >1 GPU) Splits full layers across GPUs. Good for balancing memory usage.
       - row: Splits tensors within layers by rows. Can be faster but may use more VRAM on the main GPU.
       Example: -sm layer


----- Server & API Parameters -----

--host HOST
       The IP address for the server to listen on.
       - 127.0.0.1 (default): Accessible only from your own computer.
       - 0.0.0.0: Accessible from other devices on your local network.
       Example: --host 0.0.0.0

--port PORT
       The network port for the server to listen on. Default is 8080.
       Example: --port 8080

--api-key KEY
       Protects the server with a simple password (API key). Any client connecting will need to provide this key.

--chat-template JINJA_TEMPLATE
       Specifies a chat template to use, such as 'chatml' or 'llama3'. This is crucial for formatting the prompt
       correctly for instruction-tuned models so they understand system prompts, user messages, and assistant turns.
       Example: --chat-template llama3

--no-webui
       Disables the built-in web interface. Useful if you only plan to use the API with other programs.

----- Some stuff about -ot command ------

## Mastering `llamacpp`'s `-ot` Command for Optimized Performance

For enthusiasts and practitioners leveraging the power of `llamacpp` for local large language model inference, optimizing performance, especially on systems with limited VRAM, is a paramount concern. Among the arsenal of command-line arguments available, the `-ot` (or `--override-tensor`) command stands out as a powerful tool for fine-grained control over model layer offloading. This comprehensive guide will demystify the `-ot` command, its parameters, and provide practical examples to help you unlock its full potential.

### The "-ot" Command: Precision Control Over Tensor Placement

The primary function of the `-ot` command is to override the default behavior of `llamacpp`'s layer offloading. Instead of loading entire layers onto the GPU, this command allows you to specify precisely which tensors within the model should be allocated to a different memory buffer, most commonly the CPU. This granular control is particularly beneficial when dealing with large models that would otherwise exceed your GPU's VRAM capacity. By selectively keeping larger, less computationally intensive tensors on the CPU, you can free up precious VRAM for the more critical parts of the model, often leading to significant performance improvements.

The syntax for the `-ot` command is as follows:

```
-ot <tensor_name_pattern>=<buffer_type>
```

Multiple overrides can be applied by separating them with commas or by using the `-ot` flag multiple times. The real power of this command lies in its use of C++ regular expressions (regex) for the `<tensor_name_pattern>`, enabling you to target specific tensors with surgical precision.

### Demystifying the Parameters: Tensor Naming and Regex

To effectively use the `-ot` command, it's crucial to understand the naming convention for tensors within `llamacpp` models and the basics of C++ regex.

#### Tensor Naming Convention

While a definitive, exhaustive list can be model-specific, a general and helpful naming scheme is employed in `llamacpp`:

*   **`blk.<layer_number>`**: This prefix refers to a specific block or layer within the model. The `<layer_number>` is a zero-based index. For example, `blk.0` is the first layer, `blk.1` is the second, and so on.
*   **`ffn`**: This stands for the Feed-Forward Network, a key component of each transformer block. FFN tensors are often quite large and can be good candidates for offloading to the CPU as they involve more basic matrix multiplications.
*   **`ffn_down`, `ffn_gate`, `ffn_up`**: These are sub-tensors within the Feed-Forward Network.
*   **`attn`**: This refers to the attention mechanism tensors, which are generally more computationally intensive and benefit from being on the GPU.
*   **`exps`**: This is particularly relevant for Mixture-of-Experts (MoE) models and refers to the "expert" tensors. Offloading expert tensors to the CPU can be an effective strategy for running very large MoE models on consumer hardware.

#### C++ Regex for Tensor Targeting

The `<tensor_name_pattern>` uses C++'s `std::regex` syntax. Here are some common patterns and their meanings in the context of the `-ot` command:

| Regex Pattern | Description |
| :--- | :--- |
| `.` | Matches any single character. |
| `*` | Matches the preceding element zero or more times. |
| `+` | Matches the preceding element one or more times. |
| `[set]` | Matches any single character within the set. For example, `[0-9]` matches any digit. |
| `(group)` | Groups a part of the pattern. |
| `|` | Acts as an "OR" operator, matching the expression before or after it. |
| `\` | Escapes a special character. For instance, `\.` matches a literal dot. |

### Practical Examples and Use Cases

Now, let's break down the commands from the Reddit post and explore other useful examples:

*   **`-ot blk.([8-9]|[1-9][0-9]).ffn=CPU`**: This command offloads the Feed-Forward Network (`ffn`) tensors of specific layers to the CPU. Let's dissect the regex:
    *   `blk\.`: Matches the literal string "blk.".
    *   `([8-9]|[1-9][0-9])`: This is a clever way to select layers from 8 onwards. It matches either a single digit from 8 to 9, or a two-digit number from 10 to 99.
    *   `\.ffn`: Matches the literal string ".ffn".
    *   **In essence, this command keeps the FFN tensors for layers 8 and above on the CPU.**

*   **`-ot ffn=CPU`**: This is a broader command that offloads all tensors containing "ffn" in their name to the CPU. This is a simple way to move all Feed-Forward Network components off the GPU.

*   **`-ot exps=CPU`**: As mentioned earlier, this is highly effective for Mixture-of-Experts models. It offloads all "expert" tensors to the CPU, which can dramatically reduce VRAM usage.

*   **`-ot blk.2.ffn_down_exps.weight=CPU`**: This is a very specific command that targets the `ffn_down_exps.weight` tensor within the 3rd layer (`blk.2`) and moves it to the CPU.

**More Advanced Examples:**

*   **`-ot "blk\.(1[0-9]|2[0-9]|30)\.ffn_.*=CPU"`**: This command would offload all FFN-related tensors (since `.*` matches any character zero or more times) for layers 10 through 30 to the CPU.
*   **`-ot ".*\.ffn_(gate|up)=CPU"`**: This regex targets the `ffn_gate` and `ffn_up` tensors across all layers and offloads them to the CPU, while potentially leaving the `ffn_down` tensors on the GPU.

### How to Determine Which Tensors to Offload

The decision of which tensors to offload depends on your specific hardware and the model you are running. Here's a general strategy:

1.  **Identify VRAM Bottlenecks**: If a model doesn't fit in your VRAM, the `-ot` command is your friend.
2.  **Prioritize Attention Tensors for GPU**: Attention mechanism tensors are typically more computationally intensive and benefit most from GPU acceleration.
3.  **Experiment with FFN Offloading**: Feed-Forward Network tensors are often large and less demanding on the GPU's parallel processing capabilities, making them prime candidates for CPU offloading.
4.  **Consider MoE Expert Offloading**: For Mixture-of-Experts models, offloading the expert layers (`exps`) is a very effective strategy to reduce VRAM consumption.

By understanding and strategically utilizing the `-ot` command, you can push the boundaries of your hardware and run larger, more capable language models locally with `llamacpp`. Experiment with different regex patterns and tensor targets to find the optimal configuration for your setup and enjoy a smoother, more efficient inference experience.

Sources
1. github.com
2. medium.com
3. reddit.com
4. medium.com
5. unsloth.ai
6. githubusercontent.com
7. semanticdiff.com

-----------------------------------------------------------------------------
code to print out large/all tensor for model:

# A simple script to read a GGUF model file and list its tensors by size.

from gguf import GGUFReader

# --- IMPORTANT ---
# Change this path to the full path of YOUR model file.
# Use forward slashes / instead of backslashes \.
MODEL_PATH = "D:/lm_studio/unsloth/Llama-3.3-70B-Instruct-GGUF/Llama-3.3-70B-Instruct-UD-IQ3_XXS.gguf"

print(f"Reading tensors from: {MODEL_PATH}\n")

# Open the model file for reading
reader = GGUFReader(MODEL_PATH, 'r')

# Get all the tensor info
tensors = reader.tensors

# Calculate the size (total number of parameters) for each tensor
tensor_sizes = []
for tensor in tensors:
    # Shape is a list of dimensions, e.g., [32000, 4096]
    # We multiply all dimensions to get the total parameter count
    param_count = 1
    for dim in tensor.shape:
        param_count *= dim
    tensor_sizes.append((tensor.name, param_count, tensor.shape))

# Sort the tensors from largest to smallest
sorted_tensors = sorted(tensor_sizes, key=lambda item: item[1], reverse=True)

# Print the top 10 largest tensors
print("--- Top 10 Largest Tensors ---")
for i in range(min(10, len(sorted_tensors))):
    name, size, shape = sorted_tensors[i]
    print(f"{i+1}. Name: {name:<30} | Parameters: {size:>12,} | Shape: {shape}")
# Print All
# for i in range(len(sorted_tensors)):
#     name, size, shape = sorted_tensors[i]
#     print(f"{i+1}. Name: {name:<30} | Parameters: {size:>12,} | Shape: {shape}")

-------------------------------------------------------------------------------------


Some examples: 


-ot token_embd.weight=CUDA0
put token_embd.weight onto gpu 0, normally token_embd.weight is more dense and often benefit offload to higher performing gpu

-ot blk.[0-9].ffn_gate.weight=CUDA0
put ffn_gate.weight of first 9 layer onto gpu 0

-ot blk.[1-7][0-9].ffn_gate.weight=CUDA0
put ffn_gate.weight of first 80 layer onto gpu 0

-ot blk.[0-9].ffn_up.weight=CUDA0

-ot blk.[0-9].ffn_down.weight=CUDA0




----- FULL COMMAND LIST (from llama.cpp --help) -----

--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU
                                        (env: LLAMA_ARG_CPU_MOE)
--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the
                                        CPU
                                        (env: LLAMA_ARG_N_CPU_MOE)
-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM
                                        (env: LLAMA_ARG_N_GPU_LAYERS)
-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:
                                        - none: use one GPU only
                                        - layer (default): split layers and KV across GPUs
                                        - row: split rows across GPUs
                                        (env: LLAMA_ARG_SPLIT_MODE)
-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of
                                        proportions, e.g. 3,1
                                        (env: LLAMA_ARG_TENSOR_SPLIT)
-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for
                                        intermediate results and KV (with split-mode = row) (default: 0)

-h,    --help, --usage                  print usage and exit
--version                               show version and build info
--completion-bash                       print source-able bash completion script for llama.cpp
--verbose-prompt                        print a verbose prompt before generation (default: false)
-t,    --threads N                      number of threads to use during generation (default: -1)
                                        (env: LLAMA_ARG_THREADS)
-tb,   --threads-batch N                number of threads to use during batch and prompt processing (default:
                                        same as --threads)
-C,    --cpu-mask M                     CPU affinity mask: arbitrarily long hex. Complements cpu-range
                                        (default: "")
-Cr,   --cpu-range lo-hi                range of CPUs for affinity. Complements --cpu-mask
--cpu-strict <0|1>                      use strict CPU placement (default: 0)
--prio N                                set process/thread priority : low(-1), normal(0), medium(1), high(2),
                                        realtime(3) (default: 0)
--poll <0...100>                        use polling level to wait for work (0 - no polling, default: 50)
-Cb,   --cpu-mask-batch M               CPU affinity mask: arbitrarily long hex. Complements cpu-range-batch
                                        (default: same as --cpu-mask)
-Crb,  --cpu-range-batch lo-hi          ranges of CPUs for affinity. Complements --cpu-mask-batch
--cpu-strict-batch <0|1>                use strict CPU placement (default: same as --cpu-strict)
--prio-batch N                          set process/thread priority : 0-normal, 1-medium, 2-high, 3-realtime
                                        (default: 0)
--poll-batch <0|1>                      use polling to wait for work (default: same as --poll)
-c,    --ctx-size N                     size of the prompt context (default: 4096, 0 = loaded from model)
                                        (env: LLAMA_ARG_CTX_SIZE)
-n,    --predict, --n-predict N         number of tokens to predict (default: -1, -1 = infinity)
                                        (env: LLAMA_ARG_N_PREDICT)
-b,    --batch-size N                   logical maximum batch size (default: 2048)
                                        (env: LLAMA_ARG_BATCH)
-ub,   --ubatch-size N                  physical maximum batch size (default: 512)
                                        (env: LLAMA_ARG_UBATCH)
--keep N                                number of tokens to keep from the initial prompt (default: 0, -1 =
                                        all)
--swa-full                              use full-size SWA cache (default: false)
                                        [(more
                                        info)](https://github.com/ggml-org/llama.cpp/pull/13194#issuecomment-2868343055)
                                        (env: LLAMA_ARG_SWA_FULL)
--kv-unified, -kvu                      use single unified KV buffer for the KV cache of all sequences
                                        (default: false)
                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/14363)
                                        (env: LLAMA_ARG_KV_SPLIT)
-fa,   --flash-attn                     enable Flash Attention (default: disabled)
                                        (env: LLAMA_ARG_FLASH_ATTN)
--no-perf                               disable internal libllama performance timings (default: false)
                                        (env: LLAMA_ARG_NO_PERF)
-e,    --escape                         process escapes sequences (\n, \r, \t, \', \", \\) (default: true)
--no-escape                             do not process escape sequences
--rope-scaling {none,linear,yarn}       RoPE frequency scaling method, defaults to linear unless specified by
                                        the model
                                        (env: LLAMA_ARG_ROPE_SCALING_TYPE)
--rope-scale N                          RoPE context scaling factor, expands context by a factor of N
                                        (env: LLAMA_ARG_ROPE_SCALE)
--rope-freq-base N                      RoPE base frequency, used by NTK-aware scaling (default: loaded from
                                        model)
                                        (env: LLAMA_ARG_ROPE_FREQ_BASE)
--rope-freq-scale N                     RoPE frequency scaling factor, expands context by a factor of 1/N
                                        (env: LLAMA_ARG_ROPE_FREQ_SCALE)
--yarn-orig-ctx N                       YaRN: original context size of model (default: 0 = model training
                                        context size)
                                        (env: LLAMA_ARG_YARN_ORIG_CTX)
--yarn-ext-factor N                     YaRN: extrapolation mix factor (default: -1.0, 0.0 = full
                                        interpolation)
                                        (env: LLAMA_ARG_YARN_EXT_FACTOR)
--yarn-attn-factor N                    YaRN: scale sqrt(t) or attention magnitude (default: 1.0)
                                        (env: LLAMA_ARG_YARN_ATTN_FACTOR)
--yarn-beta-slow N                      YaRN: high correction dim or alpha (default: 1.0)
                                        (env: LLAMA_ARG_YARN_BETA_SLOW)
--yarn-beta-fast N                      YaRN: low correction dim or beta (default: 32.0)
                                        (env: LLAMA_ARG_YARN_BETA_FAST)
-nkvo, --no-kv-offload                  disable KV offload
                                        (env: LLAMA_ARG_NO_KV_OFFLOAD)
-nr,   --no-repack                      disable weight repacking
                                        (env: LLAMA_ARG_NO_REPACK)
-ctk,  --cache-type-k TYPE              KV cache data type for K
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_K)
-ctv,  --cache-type-v TYPE              KV cache data type for V
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_V)
-dt,   --defrag-thold N                 KV cache defragmentation threshold (DEPRECATED)
                                        (env: LLAMA_ARG_DEFRAG_THOLD)
-np,   --parallel N                     number of parallel sequences to decode (default: 1)
                                        (env: LLAMA_ARG_N_PARALLEL)
--rpc SERVERS                           comma separated list of RPC servers
                                        (env: LLAMA_ARG_RPC)
--mlock                                 force system to keep model in RAM rather than swapping or compressing
                                        (env: LLAMA_ARG_MLOCK)
--no-mmap                               do not memory-map model (slower load but may reduce pageouts if not
                                        using mlock)
                                        (env: LLAMA_ARG_NO_MMAP)
--numa TYPE                             attempt optimizations that help on some NUMA systems
                                        - distribute: spread execution evenly over all nodes
                                        - isolate: only spawn threads on CPUs on the node that execution
                                        started on
                                        - numactl: use the CPU map provided by numactl
                                        if run without this previously, it is recommended to drop the system
                                        page cache before using this
                                        see https://github.com/ggml-org/llama.cpp/issues/1437
                                        (env: LLAMA_ARG_NUMA)
-dev,  --device <dev1,dev2,..>          comma-separated list of devices to use for offloading (none = don't
                                        offload)
                                        use --list-devices to see a list of available devices
                                        (env: LLAMA_ARG_DEVICE)
--list-devices                          print list of available devices and exit
--override-tensor, -ot <tensor name pattern>=<buffer type>,...
                                        override tensor buffer type
--cpu-moe, -cmoe                        keep all Mixture of Experts (MoE) weights in the CPU
                                        (env: LLAMA_ARG_CPU_MOE)
--n-cpu-moe, -ncmoe N                   keep the Mixture of Experts (MoE) weights of the first N layers in the
                                        CPU
                                        (env: LLAMA_ARG_N_CPU_MOE)
-ngl,  --gpu-layers, --n-gpu-layers N   number of layers to store in VRAM
                                        (env: LLAMA_ARG_N_GPU_LAYERS)
-sm,   --split-mode {none,layer,row}    how to split the model across multiple GPUs, one of:
                                        - none: use one GPU only
                                        - layer (default): split layers and KV across GPUs
                                        - row: split rows across GPUs
                                        (env: LLAMA_ARG_SPLIT_MODE)
-ts,   --tensor-split N0,N1,N2,...      fraction of the model to offload to each GPU, comma-separated list of
                                        proportions, e.g. 3,1
                                        (env: LLAMA_ARG_TENSOR_SPLIT)
-mg,   --main-gpu INDEX                 the GPU to use for the model (with split-mode = none), or for
                                        intermediate results and KV (with split-mode = row) (default: 0)
                                        (env: LLAMA_ARG_MAIN_GPU)
--check-tensors                         check model tensor data for invalid values (default: false)
--override-kv KEY=TYPE:VALUE            advanced option to override model metadata by key. may be specified
                                        multiple times.
                                        types: int, float, bool, str. example: --override-kv
                                        tokenizer.ggml.add_bos_token=bool:false
--no-op-offload                         disable offloading host tensor operations to device (default: false)
--lora FNAME                            path to LoRA adapter (can be repeated to use multiple adapters)
--lora-scaled FNAME SCALE               path to LoRA adapter with user defined scaling (can be repeated to use
                                        multiple adapters)
--control-vector FNAME                  add a control vector
                                        note: this argument can be repeated to add multiple control vectors
--control-vector-scaled FNAME SCALE     add a control vector with user defined scaling SCALE
                                        note: this argument can be repeated to add multiple scaled control
                                        vectors
--control-vector-layer-range START END
                                        layer range to apply the control vector(s) to, start and end inclusive
-m,    --model FNAME                    model path (default: `models/$filename` with filename from `--hf-file`
                                        or `--model-url` if set, otherwise models/7B/ggml-model-f16.gguf)
                                        (env: LLAMA_ARG_MODEL)
-mu,   --model-url MODEL_URL            model download url (default: unused)
                                        (env: LLAMA_ARG_MODEL_URL)
-hf,   -hfr, --hf-repo <user>/<model>[:quant]
                                        Hugging Face model repository; quant is optional, case-insensitive,
                                        default to Q4_K_M, or falls back to the first file in the repo if
                                        Q4_K_M doesn't exist.
                                        mmproj is also downloaded automatically if available. to disable, add
                                        --no-mmproj
                                        example: unsloth/phi-4-GGUF:q4_k_m
                                        (default: unused)
                                        (env: LLAMA_ARG_HF_REPO)
-hfd,  -hfrd, --hf-repo-draft <user>/<model>[:quant]
                                        Same as --hf-repo, but for the draft model (default: unused)
                                        (env: LLAMA_ARG_HFD_REPO)
-hff,  --hf-file FILE                   Hugging Face model file. If specified, it will override the quant in
                                        --hf-repo (default: unused)
                                        (env: LLAMA_ARG_HF_FILE)
-hfv,  -hfrv, --hf-repo-v <user>/<model>[:quant]
                                        Hugging Face model repository for the vocoder model (default: unused)
                                        (env: LLAMA_ARG_HF_REPO_V)
-hffv, --hf-file-v FILE                 Hugging Face model file for the vocoder model (default: unused)
                                        (env: LLAMA_ARG_HF_FILE_V)
-hft,  --hf-token TOKEN                 Hugging Face access token (default: value from HF_TOKEN environment
                                        variable)
                                        (env: HF_TOKEN)
--log-disable                           Log disable
--log-file FNAME                        Log to file
--log-colors                            Enable colored logging
                                        (env: LLAMA_LOG_COLORS)
-v,    --verbose, --log-verbose         Set verbosity level to infinity (i.e. log all messages, useful for
                                        debugging)
--offline                               Offline mode: forces use of cache, prevents network access
                                        (env: LLAMA_OFFLINE)
-lv,   --verbosity, --log-verbosity N   Set the verbosity threshold. Messages with a higher verbosity will be
                                        ignored.
                                        (env: LLAMA_LOG_VERBOSITY)
--log-prefix                            Enable prefix in log messages
                                        (env: LLAMA_LOG_PREFIX)
--log-timestamps                        Enable timestamps in log messages
                                        (env: LLAMA_LOG_TIMESTAMPS)
-ctkd, --cache-type-k-draft TYPE        KV cache data type for K for the draft model
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_K_DRAFT)
-ctvd, --cache-type-v-draft TYPE        KV cache data type for V for the draft model
                                        allowed values: f32, f16, bf16, q8_0, q4_0, q4_1, iq4_nl, q5_0, q5_1
                                        (default: f16)
                                        (env: LLAMA_ARG_CACHE_TYPE_V_DRAFT)


----- sampling params -----

--samplers SAMPLERS                     samplers that will be used for generation in the order, separated by
                                        ';'
                                        (default:
                                        penalties;dry;top_n_sigma;top_k;typ_p;top_p;min_p;xtc;temperature)
-s,    --seed SEED                      RNG seed (default: -1, use random seed for -1)
--sampling-seq, --sampler-seq SEQUENCE
                                        simplified sequence for samplers that will be used (default:
                                        edskypmxt)
--ignore-eos                            ignore end of stream token and continue generating (implies
                                        --logit-bias EOS-inf)
--temp N                                temperature (default: 0.8)
--top-k N                               top-k sampling (default: 40, 0 = disabled)
--top-p N                               top-p sampling (default: 0.9, 1.0 = disabled)
--min-p N                               min-p sampling (default: 0.1, 0.0 = disabled)
--top-nsigma N                          top-n-sigma sampling (default: -1.0, -1.0 = disabled)
--xtc-probability N                     xtc probability (default: 0.0, 0.0 = disabled)
--xtc-threshold N                       xtc threshold (default: 0.1, 1.0 = disabled)
--typical N                             locally typical sampling, parameter p (default: 1.0, 1.0 = disabled)
--repeat-last-n N                       last n tokens to consider for penalize (default: 64, 0 = disabled, -1
                                        = ctx_size)
--repeat-penalty N                      penalize repeat sequence of tokens (default: 1.0, 1.0 = disabled)
--presence-penalty N                    repeat alpha presence penalty (default: 0.0, 0.0 = disabled)
--frequency-penalty N                   repeat alpha frequency penalty (default: 0.0, 0.0 = disabled)
--dry-multiplier N                      set DRY sampling multiplier (default: 0.0, 0.0 = disabled)
--dry-base N                            set DRY sampling base value (default: 1.75)
--dry-allowed-length N                  set allowed length for DRY sampling (default: 2)
--dry-penalty-last-n N                  set DRY penalty for the last n tokens (default: -1, 0 = disable, -1 =
                                        context size)
--dry-sequence-breaker STRING           add sequence breaker for DRY sampling, clearing out default breakers
                                        ('\n', ':', '"', '*') in the process; use "none" to not use any
                                        sequence breakers
--dynatemp-range N                      dynamic temperature range (default: 0.0, 0.0 = disabled)
--dynatemp-exp N                        dynamic temperature exponent (default: 1.0)
--mirostat N                            use Mirostat sampling.
                                        Top K, Nucleus and Locally Typical samplers are ignored if used.
                                        (default: 0, 0 = disabled, 1 = Mirostat, 2 = Mirostat 2.0)
--mirostat-lr N                         Mirostat learning rate, parameter eta (default: 0.1)
--mirostat-ent N                        Mirostat target entropy, parameter tau (default: 5.0)
-l,    --logit-bias TOKEN_ID(+/-)BIAS   modifies the likelihood of token appearing in the completion,
                                        i.e. `--logit-bias 15043+1` to increase likelihood of token ' Hello',
                                        or `--logit-bias 15043-1` to decrease likelihood of token ' Hello'
--grammar GRAMMAR                       BNF-like grammar to constrain generations (see samples in grammars/
                                        dir) (default: '')
--grammar-file FNAME                    file to read grammar from
-j,    --json-schema SCHEMA             JSON schema to constrain generations (https://json-schema.org/), e.g.
                                        `{}` for any JSON object
                                        For schemas w/ external $refs, use --grammar +
                                        example/json_schema_to_grammar.py instead
-jf,   --json-schema-file FILE          File containing a JSON schema to constrain generations
                                        (https://json-schema.org/), e.g. `{}` for any JSON object
                                        For schemas w/ external $refs, use --grammar +
                                        example/json_schema_to_grammar.py instead


----- example-specific params -----

--swa-checkpoints N                     max number of SWA checkpoints per slot to create (default: 3)
                                        [(more info)](https://github.com/ggml-org/llama.cpp/pull/15293)
                                        (env: LLAMA_ARG_SWA_CHECKPOINTS)
--no-context-shift                      disables context shift on infinite text generation (default: enabled)
                                        (env: LLAMA_ARG_NO_CONTEXT_SHIFT)
--context-shift                         enables context shift on infinite text generation (default: disabled)
                                        (env: LLAMA_ARG_CONTEXT_SHIFT)
-r,    --reverse-prompt PROMPT          halt generation at PROMPT, return control in interactive mode
-sp,   --special                        special tokens output enabled (default: false)
--no-warmup                             skip warming up the model with an empty run
--spm-infill                            use Suffix/Prefix/Middle pattern for infill (instead of
                                        Prefix/Suffix/Middle) as some models prefer this. (default: disabled)
--pooling {none,mean,cls,last,rank}     pooling type for embeddings, use model default if unspecified
                                        (env: LLAMA_ARG_POOLING)
-cb,   --cont-batching                  enable continuous batching (a.k.a dynamic batching) (default: enabled)
                                        (env: LLAMA_ARG_CONT_BATCHING)
-nocb, --no-cont-batching               disable continuous batching
                                        (env: LLAMA_ARG_NO_CONT_BATCHING)
--mmproj FILE                           path to a multimodal projector file. see tools/mtmd/README.md
                                        note: if -hf is used, this argument can be omitted
                                        (env: LLAMA_ARG_MMPROJ)
--mmproj-url URL                        URL to a multimodal projector file. see tools/mtmd/README.md
                                        (env: LLAMA_ARG_MMPROJ_URL)
--no-mmproj                             explicitly disable multimodal projector, useful when using -hf
                                        (env: LLAMA_ARG_NO_MMPROJ)
--no-mmproj-offload                     do not offload multimodal projector to GPU
                                        (env: LLAMA_ARG_NO_MMPROJ_OFFLOAD)
--override-tensor-draft, -otd <tensor name pattern>=<buffer type>,...
                                        override tensor buffer type for draft model
--cpu-moe-draft, -cmoed                 keep all Mixture of Experts (MoE) weights in the CPU for the draft
                                        model
                                        (env: LLAMA_ARG_CPU_MOE_DRAFT)
--n-cpu-moe-draft, -ncmoed N            keep the Mixture of Experts (MoE) weights of the first N layers in the
                                        CPU for the draft model
                                        (env: LLAMA_ARG_N_CPU_MOE_DRAFT)
-a,    --alias STRING                   set alias for model name (to be used by REST API)
                                        (env: LLAMA_ARG_ALIAS)
--host HOST                             ip address to listen, or bind to an UNIX socket if the address ends
                                        with .sock (default: 127.0.0.1)
                                        (env: LLAMA_ARG_HOST)
--port PORT                             port to listen (default: 8080)
                                        (env: LLAMA_ARG_PORT)
--path PATH                             path to serve static files from (default: )
                                        (env: LLAMA_ARG_STATIC_PATH)
--api-prefix PREFIX                     prefix path the server serves from, without the trailing slash
                                        (default: )
                                        (env: LLAMA_ARG_API_PREFIX)
--no-webui                              Disable the Web UI (default: enabled)
                                        (env: LLAMA_ARG_NO_WEBUI)
--embedding, --embeddings               restrict to only support embedding use case; use only with dedicated
                                        embedding models (default: disabled)
                                        (env: LLAMA_ARG_EMBEDDINGS)
--reranking, --rerank                   enable reranking endpoint on server (default: disabled)
                                        (env: LLAMA_ARG_RERANKING)
--api-key KEY                           API key to use for authentication (default: none)
                                        (env: LLAMA_API_KEY)
--api-key-file FNAME                    path to file containing API keys (default: none)
--ssl-key-file FNAME                    path to file a PEM-encoded SSL private key
                                        (env: LLAMA_ARG_SSL_KEY_FILE)
--ssl-cert-file FNAME                   path to file a PEM-encoded SSL certificate
                                        (env: LLAMA_ARG_SSL_CERT_FILE)
--chat-template-kwargs STRING           sets additional params for the json template parser
                                        (env: LLAMA_CHAT_TEMPLATE_KWARGS)
-to,   --timeout N                      server read/write timeout in seconds (default: 600)
                                        (env: LLAMA_ARG_TIMEOUT)
--threads-http N                        number of threads used to process HTTP requests (default: -1)
                                        (env: LLAMA_ARG_THREADS_HTTP)
--cache-reuse N                         min chunk size to attempt reusing from the cache via KV shifting
                                        (default: 0)
                                        [(card)](https://ggml.ai/f0.png)
                                        (env: LLAMA_ARG_CACHE_REUSE)
--metrics                               enable prometheus compatible metrics endpoint (default: disabled)
                                        (env: LLAMA_ARG_ENDPOINT_METRICS)
--slots                                 enable slots monitoring endpoint (default: disabled)
                                        (env: LLAMA_ARG_ENDPOINT_SLOTS)
--props                                 enable changing global properties via POST /props (default: disabled)
                                        (env: LLAMA_ARG_ENDPOINT_PROPS)
--no-slots                              disables slots monitoring endpoint
                                        (env: LLAMA_ARG_NO_ENDPOINT_SLOTS)
--slot-save-path PATH                   path to save slot kv cache (default: disabled)
--jinja                                 use jinja template for chat (default: disabled)
                                        (env: LLAMA_ARG_JINJA)
--reasoning-format FORMAT               controls whether thought tags are allowed and/or extracted from the
                                        response, and in which format they're returned; one of:
                                        - none: leaves thoughts unparsed in `message.content`
                                        - deepseek: puts thoughts in `message.reasoning_content` (except in
                                        streaming mode, which behaves as `none`)
                                        (default: auto)
                                        (env: LLAMA_ARG_THINK)
--reasoning-budget N                    controls the amount of thinking allowed; currently only one of: -1 for
                                        unrestricted thinking budget, or 0 to disable thinking (default: -1)
                                        (env: LLAMA_ARG_THINK_BUDGET)
--chat-template JINJA_TEMPLATE          set custom jinja chat template (default: template taken from model's
                                        metadata)
                                        if suffix/prefix are specified, template will be disabled
                                        only commonly used templates are accepted (unless --jinja is set
                                        before this flag):
                                        list of built-in templates:
                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,
                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,
                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,
                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,
                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,
                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,
                                        seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr
                                        (env: LLAMA_ARG_CHAT_TEMPLATE)
--chat-template-file JINJA_TEMPLATE_FILE
                                        set custom jinja chat template file (default: template taken from
                                        model's metadata)
                                        if suffix/prefix are specified, template will be disabled
                                        only commonly used templates are accepted (unless --jinja is set
                                        before this flag):
                                        list of built-in templates:
                                        bailing, chatglm3, chatglm4, chatml, command-r, deepseek, deepseek2,
                                        deepseek3, exaone3, exaone4, falcon3, gemma, gigachat, glmedge,
                                        gpt-oss, granite, hunyuan-dense, hunyuan-moe, kimi-k2, llama2,
                                        llama2-sys, llama2-sys-bos, llama2-sys-strip, llama3, llama4, megrez,
                                        minicpm, mistral-v1, mistral-v3, mistral-v3-tekken, mistral-v7,
                                        mistral-v7-tekken, monarch, openchat, orion, phi3, phi4, rwkv-world,
                                        seed_oss, smolvlm, vicuna, vicuna-orca, yandex, zephyr
                                        (env: LLAMA_ARG_CHAT_TEMPLATE_FILE)
--no-prefill-assistant                  whether to prefill the assistant's response if the last message is an
                                        assistant message (default: prefill enabled)
                                        when this flag is set, if the last message is an assistant message
                                        then it will be treated as a full message and not prefilled

                                        (env: LLAMA_ARG_NO_PREFILL_ASSISTANT)
-sps,  --slot-prompt-similarity SIMILARITY
                                        how much the prompt of a request must match the prompt of a slot in
                                        order to use that slot (default: 0.50, 0.0 = disabled)
--lora-init-without-apply               load LoRA adapters without applying them (apply later via POST
                                        /lora-adapters) (default: disabled)
-td,   --threads-draft N                number of threads to use during generation (default: same as
                                        --threads)
-tbd,  --threads-batch-draft N          number of threads to use during batch and prompt processing (default:
                                        same as --threads-draft)
--draft-max, --draft, --draft-n N       number of tokens to draft for speculative decoding (default: 16)
                                        (env: LLAMA_ARG_DRAFT_MAX)
--draft-min, --draft-n-min N            minimum number of draft tokens to use for speculative decoding
                                        (default: 0)
                                        (env: LLAMA_ARG_DRAFT_MIN)
--draft-p-min P                         minimum speculative decoding probability (greedy) (default: 0.8)
                                        (env: LLAMA_ARG_DRAFT_P_MIN)
-cd,   --ctx-size-draft N               size of the prompt context for the draft model (default: 0, 0 = loaded
                                        from model)
                                        (env: LLAMA_ARG_CTX_SIZE_DRAFT)
-devd, --device-draft <dev1,dev2,..>    comma-separated list of devices to use for offloading the draft model
                                        (none = don't offload)
                                        use --list-devices to see a list of available devices
-ngld, --gpu-layers-draft, --n-gpu-layers-draft N
                                        number of layers to store in VRAM for the draft model
                                        (env: LLAMA_ARG_N_GPU_LAYERS_DRAFT)
-md,   --model-draft FNAME              draft model for speculative decoding (default: unused)
                                        (env: LLAMA_ARG_MODEL_DRAFT)
--spec-replace TARGET DRAFT             translate the string in TARGET into DRAFT if the draft model and main
                                        model are not compatible
-mv,   --model-vocoder FNAME            vocoder model for audio generation (default: unused)
--tts-use-guide-tokens                  Use guide tokens to improve TTS word recall
--embd-bge-small-en-default             use default bge-small-en-v1.5 model (note: can download weights from
                                        the internet)
--embd-e5-small-en-default              use default e5-small-v2 model (note: can download weights from the
                                        internet)
--embd-gte-small-default                use default gte-small model (note: can download weights from the
                                        internet)
--fim-qwen-1.5b-default                 use default Qwen 2.5 Coder 1.5B (note: can download weights from the
                                        internet)
--fim-qwen-3b-default                   use default Qwen 2.5 Coder 3B (note: can download weights from the
                                        internet)
--fim-qwen-7b-default                   use default Qwen 2.5 Coder 7B (note: can download weights from the
                                        internet)
--fim-qwen-7b-spec                      use Qwen 2.5 Coder 7B + 0.5B draft for speculative decoding (note: can
                                        download weights from the internet)
--fim-qwen-14b-spec                     use Qwen 2.5 Coder 14B + 0.5B draft for speculative decoding (note:
                                        can download weights from the internet)
--fim-qwen-30b-default                  use default Qwen 3 Coder 30B A3B Instruct (note: can download weights
                                        from the internet)
										
-----Documentation-----

## Llama.cpp Model Launcher: User Guide

### 1. Introduction

Welcome to the Llama.cpp Model Launcher! This application provides a clean, powerful, and user-friendly graphical interface (GUI) for the `llama-server.exe` tool from the Llama.cpp project.

Its purpose is to replace the tedious and error-prone process of typing long commands into a terminal. With this launcher, you can manage, edit, delete, duplicate and run all your language models with the point-and-click simplicity of a modern desktop application.

### 2. First-Time Setup

Before you can launch a model, you need to tell the application where to find two key items. This is a one-time setup, and your choices will be saved for future sessions.

1.  **Set the Llama.cpp Directory**:
    *   Click the **Browse...** button next to the "Llama.cpp Directory" label.
    *   Navigate to and select the folder that contains your `llama-server.exe` file.
    *   The application will verify that `llama-server.exe` exists in the selected folder.

2.  **Set the Models File**:
    *   Click the **Browse...** button next to the "Models File" label.
    *   Select the `.txt` file that contains your model launch commands.
    *   **File Format**: This text file must be structured with a model name on one line, followed immediately by its full launch command on the next line. For example:
        ```
		Llama-3.3-70B-Instruct-UD-IQ3_XXS
		llama-server.exe -m D:\lm_studio\unsloth\Llama-3.3-70B-Instruct-GGUF\Llama-3.3-70B-Instruct-UD-IQ3_XXS.gguf -md D:\lm_studio\lmstudio-community\Llama-3.2-1B-Instruct-GGUF\Llama-3.2-1B-Instruct-Q4_K_M.gguf --jinja -c 13000 -ngld 99 -ngl 99 -fa on --temp 0.8 --top-k 40 --top-p 0.95 --min-p 0.05 --repeat-penalty 1.1 --cache-type-k q8_0 --cache-type-v q8_0 --cache-type-k-draft q8_0 --cache-type-v-draft q8_0 --no-mmap -ts 63/18 -t 8 --device-draft CUDA0 --main-gpu 0 --no-warmup --override-tensor token_embd.weight=CUDA0

        gpt-oss-20b-MXFP4
		llama-server.exe -m D:\lm_studio\lmstudio-community\gpt-oss-20b-GGUF\gpt-oss-20b-MXFP4.gguf --jinja -c 131000 -ngl 999 -fa on --temp 1.0 --top-k 100 --top-p 1.0 --min-p 0.05 --repeat-penalty 1.1 --no-mmap --split-mode none --main-gpu 0 --chat-template-kwargs "{\"reasoning_effort\": \"medium\"}" --no-warmup --parallel 1 --ubatch-size 4096 --batch-size 8192
        ```

Once both paths are set, the **Model Selection** dropdown menu will automatically populate with the names from your text file.

### 3. The Main Interface

The application is divided into two main panels.

#### Left Panel: Main Control & Display
This is where you select and control the model server.

*   **Model Selection Dropdown**: Choose the model configuration you wish to load.
*   **Web UI Options**:
    *   `Enable Web UI`: Keep this checked to run the standard web server. Unchecking it adds the `--no-webui` flag to the command.
    *   `Auto-Open Web UI`: If checked, your web browser will automatically open to the server's page (`http://localhost:8080`) a single time after the model successfully loads. It will then be unchecked so no more pages will open UNLESS you re-check it or relaunch the app.
*   **Process Control Buttons**:
    *   **Load Model**: Builds the final command from the editor and starts the `llama-server.exe` process.
    *   **Unload Model**: Forcefully stops the server process.
    *   **Exit**: Stops any running server and closes the application.
*   **Status Indicator**: A colored dot gives you an at-a-glance view of the server's state:
    *   **Red (Unloaded)**: The server is not running.
    *   **Yellow (Loading...)**: The server has started and is loading the model into memory.
    *   **Green (Loaded)**: The model is successfully loaded and ready to accept requests.
    *   **Red (Error)**: The server process terminated unexpectedly (e.g., due to a bad parameter).
*   **Output / Commands View**:
    *   The main text area shows the **live output** from the server by default, which is useful for monitoring loading progress and API requests.
    *   Click the **Commands** button to switch the view. It will load and display the content of a text file named **`models_commands.txt`**, which should be located in the same directory as the text file you selected for your "Models File". This reference file contains a comprehensive list of commands from the Llama.cpp project and can be customized with your own notes. Click **Show Output** to return to the live log.

#### Right Panel: Configuration Editor
This is where you can view and modify all aspects of the selected model's configuration.

*   **Model Name**: An editable field for the display name that appears in the dropdown.
*   **Parameter Editor**: A dynamic list of all parameters for the selected command.
    *   Flags (like `--no-mmap`) are shown as **checkboxes**. Uncheck to disable them.
    *   Parameters with values (like `-c 4096`) are shown as **text fields**.
*   **Add New Parameter**: Allows you to add any valid Llama.cpp parameter to the current configuration on the fly.
*   **Action Buttons**:
    *   **Add Model**: Prepares the editor for a new model configuration using a default template.
    *   **Delete Model**: Deletes the currently selected model from your `.txt` file.
    *   **Reset**: Discards any changes made in the editor and reloads the parameters for the selected model that is CURRENTLY saved in models text file.
    *   **Save to File**: Permanently saves all changes (name and parameters) to your `.txt` file.

### 4. Core Workflow: Launching a Model

1.  **Select a Model**: Choose a model from the dropdown menu on the left.
2.  **Tune Parameters**: The model's parameters will appear in the editor on the right. You can:
    *   Change the context size by editing the value for the `-c` parameter.
    *   Enable or disable a flag by checking or unchecking its box.
    *   Remove a parameter by clicking the **"X"** button next to it.
    *   Add a new one, like `--temp 0.7`, using the "Add New Parameter" section.
3.  **Launch the Model**: Click the **Load Model** button.
4.  **Monitor Progress**: The Status Indicator will turn yellow, and the Output View will show the real-time log from Llama.cpp as it loads the model.
5.  **Use the Model**: Once the log indicates the server is "listening" and the Status Indicator turns green, the model is ready. If "Auto-Open Web UI" was checked, a browser tab will open automatically.
6.  **Shut Down**: When finished, click **Unload Model** to stop the server or **Exit** to close the application.

### 5. Managing Your Models (alway keep a backup of Model File..just in case)

The launcher's configuration management features allow you to modify your model library without manually editing text files.

#### To Edit an Existing Model:

1.  Select the model from the dropdown.
2.  In the right panel, change the **Model Name** and/or any of its **parameters**.
3.  Click **Save to File**. The old name and command will be updated in your `.txt` file.

#### To Add a New Model:

1.  Click the **Add Model** button.
2.  The editor will be populated with a default template and a unique name (e.g., "New Model 1").
3.  **Crucially, edit the `-m` parameter** to point to the correct `.gguf` file path for your new model.
4.  Change the **Model Name** to something descriptive.
5.  Adjust other parameters as needed.
6.  Click **Save to File**. The new configuration will be added to the end of your `.txt` file.

#### To Duplicate a Model:

1.  Select the model you wish to duplicate from the dropdown.
2.  Click the **Duplicate** button.
3.  A duplicate model is created, you can delete/add parameters.
4.  Don't forget to click save to file if you like the changes.

#### To Delete a Model:

1.  Select the model you wish to remove from the dropdown.
2.  Click the **Delete Model** button.
3.  A confirmation dialog will appear. Click **Yes** to proceed.
4.  The model's name and its command will be permanently removed from your `.txt` file.

### 6. Running the Application

There are two primary ways to run this application:

#### Method 1: Run from Python Source

This method is ideal for developers or users who have Python installed and are comfortable with a code editor.

1.  **Install Dependencies**: The application requires the PyQt6 library. Install it using pip:
    ```bash
    pip install PyQt6
    ```
2.  **Run the Script**: Save the application code as a Python file (e.g., `launcher.py`) and run it from your terminal or preferred code editor.

#### Method 2: Compile to a Standalone Executable (.exe)

This method packages the application into a single `.exe` file that can be run on any Windows machine without needing Python installed.

1.  **Install PyInstaller**: This module handles the compilation process. Install it using pip:
    ```bash
    pip install pyinstaller
    ```
2.  **Run the Command**: Open a terminal in the directory where you saved the Python script. Run the following command:
    ```bash
    pyinstaller --onefile --windowed --icon=C:\path\to\your\icon.ico your_script_name.py
    ```
    *   `--onefile`: Packages everything into a single executable file.
    *   `--windowed`: Prevents a console window from appearing when you run the app.
    *   `--icon`: (Optional) Sets a custom icon for the executable. You can omit this flag if you don't have an `.ico` file.

After the command completes, you will find your standalone `.exe` file inside a new `dist` folder.


Common Issue:
When you click load model and nothing happens:
This usually means it doesnt know where the llamacpp directory is. This usually happens to me
when I update to a newer release of llamacpp. Browse to the new llamacpp directory.
I am on window 10 so normally I just download the newest binary release of llamacpp, exact it
and point the app to the extract folder.
